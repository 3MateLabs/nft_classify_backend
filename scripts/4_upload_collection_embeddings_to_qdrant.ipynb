{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script to upload collection-specific NPZ-stored embeddings to Qdrant efficiently\n",
    "This script works with the NPZ file format created by generate_collection_embeddings_npz.py\n",
    "\"\"\"\n",
    "\n",
    "# Import required libraries\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import concurrent.futures\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "import queue\n",
    "import threading\n",
    "import uuid\n",
    "import hashlib\n",
    "\n",
    "# Add the project root to the path so we can import from api\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "try:\n",
    "    from api.config import EMBEDDING_DIR\n",
    "except ImportError:\n",
    "    # Fallback if import fails\n",
    "    EMBEDDING_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), \"data\", \"embeddings\")\n",
    "    print(f\"Using fallback EMBEDDING_DIR: {EMBEDDING_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qdrant configuration\n",
    "QDRANT_API_URL = \"https://55daf392-afac-492f-bf66-2871e1510fc7.us-east4-0.gcp.cloud.qdrant.io:6333\"\n",
    "QDRANT_API_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3MiOiJtIn0.2FwGSL4xcHHqtrNJ3-Nffi6Ext0qpI5VzC9MrK153io\"\n",
    "\n",
    "# Constants\n",
    "COLLECTION_NAME = \"nft_embeddings\"  # Name of the collection in Qdrant\n",
    "BATCH_SIZE = 100  # Number of points to upload in a single request\n",
    "MAX_WORKERS = 10  # Number of concurrent workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_collection_exists(collection_name: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a collection exists in Qdrant\n",
    "    \n",
    "    Args:\n",
    "        collection_name: Name of the collection to check\n",
    "        \n",
    "    Returns:\n",
    "        True if collection exists, False otherwise\n",
    "    \"\"\"\n",
    "    url = f\"{QDRANT_API_URL}/collections/{collection_name}\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": QDRANT_API_KEY\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        return response.status_code == 200\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking if collection exists: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def create_collection(collection_name: str, vector_size: int = 768) -> bool:\n",
    "    \"\"\"\n",
    "    Create a new collection in Qdrant\n",
    "    \n",
    "    Args:\n",
    "        collection_name: Name of the collection to create\n",
    "        vector_size: Dimensionality of the vectors to store\n",
    "        \n",
    "    Returns:\n",
    "        True if collection was created successfully, False otherwise\n",
    "    \"\"\"\n",
    "    url = f\"{QDRANT_API_URL}/collections/{collection_name}\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": QDRANT_API_KEY\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"vectors\": {\n",
    "            \"size\": vector_size,\n",
    "            \"distance\": \"Cosine\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.put(url, json=payload, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        print(f\"Successfully created collection {collection_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating collection: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_points(collection_name: str, points: List[Dict[str, Any]]) -> bool:\n",
    "    \"\"\"\n",
    "    Upload points to Qdrant\n",
    "    \n",
    "    Args:\n",
    "        collection_name: Name of the collection to upload to\n",
    "        points: List of points to upload\n",
    "        \n",
    "    Returns:\n",
    "        True if upload was successful, False otherwise\n",
    "    \"\"\"\n",
    "    url = f\"{QDRANT_API_URL}/collections/{collection_name}/points\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": QDRANT_API_KEY\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"points\": points\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Print the first point for debugging\n",
    "        if points:\n",
    "            print(f\"\\nSample point being sent to Qdrant:\")\n",
    "            sample_point = points[0].copy()\n",
    "            if 'vector' in sample_point:\n",
    "                sample_point['vector'] = f\"[vector with {len(sample_point['vector'])} dimensions]\"\n",
    "            print(json.dumps(sample_point, indent=2))\n",
    "            \n",
    "        response = requests.put(url, json=payload, headers=headers)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error response from Qdrant: {response.status_code}\")\n",
    "            print(f\"Response content: {response.text}\")\n",
    "            \n",
    "        response.raise_for_status()\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading points: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def hex_to_uuid(hex_string: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert a hex string to a valid UUID\n",
    "    \n",
    "    Args:\n",
    "        hex_string: Hex string to convert\n",
    "        \n",
    "    Returns:\n",
    "        UUID string\n",
    "    \"\"\"\n",
    "    # Remove '0x' prefix if present\n",
    "    if hex_string.startswith('0x'):\n",
    "        hex_string = hex_string[2:]\n",
    "    \n",
    "    # Generate a deterministic UUID based on the hex string\n",
    "    # Use MD5 hash to ensure consistent length and format\n",
    "    md5_hash = hashlib.md5(hex_string.encode()).hexdigest()\n",
    "    \n",
    "    # Format as UUID\n",
    "    return str(uuid.UUID(md5_hash))\n",
    "\n",
    "def upload_worker(collection_name: str, task_queue: queue.Queue, progress: Dict[str, Any]):\n",
    "    \"\"\"\n",
    "    Worker function to upload batches of points to Qdrant\n",
    "    \n",
    "    Args:\n",
    "        collection_name: Name of the collection to upload to\n",
    "        task_queue: Queue containing batches to upload\n",
    "        progress: Dictionary for tracking progress\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            batch = task_queue.get(block=False)\n",
    "            if batch is None:  # Sentinel value to stop the worker\n",
    "                break\n",
    "                \n",
    "            success = upload_points(collection_name, batch)\n",
    "            with progress[\"lock\"]:\n",
    "                if success:\n",
    "                    progress[\"success\"] += len(batch)\n",
    "                else:\n",
    "                    progress[\"failed\"] += len(batch)\n",
    "                progress[\"pbar\"].update(len(batch))\n",
    "                \n",
    "        except queue.Empty:\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error in worker: {str(e)}\")\n",
    "        finally:\n",
    "            task_queue.task_done()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_available_collections() -> List[str]:\n",
    "    \"\"\"\n",
    "    List available collections with NPZ and metadata files\n",
    "    \n",
    "    Returns:\n",
    "        List of collection names\n",
    "    \"\"\"\n",
    "    collections = []\n",
    "    \n",
    "    # Get all files in the embeddings directory\n",
    "    files = os.listdir(EMBEDDING_DIR)\n",
    "    \n",
    "    # Find NPZ files\n",
    "    npz_files = [f for f in files if f.endswith('_embeddings.npz')]\n",
    "    \n",
    "    # Extract collection names\n",
    "    for npz_file in npz_files:\n",
    "        collection_name = npz_file.replace('_embeddings.npz', '')\n",
    "        metadata_file = f\"{collection_name}_metadata.json\"\n",
    "        \n",
    "        # Check if both NPZ and metadata files exist\n",
    "        if metadata_file in files:\n",
    "            collections.append(collection_name)\n",
    "    \n",
    "    return collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections = list_available_collections()\n",
    "print(f\"\\nAvailable collections ({len(collections)}):\\n\")\n",
    "for i, collection in enumerate(collections):\n",
    "    print(f\"{i+1}. {collection}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the collection name here - replace with your desired collection\n",
    "collection_name = \"DoubleUp Citizen\"  # Example - change this to your target collection\n",
    "\n",
    "# Check if NPZ file and metadata file exist\n",
    "npz_filename = f\"{collection_name}_embeddings.npz\"\n",
    "metadata_filename = f\"{collection_name}_metadata.json\"\n",
    "\n",
    "npz_path = os.path.join(EMBEDDING_DIR, npz_filename)\n",
    "metadata_path = os.path.join(EMBEDDING_DIR, metadata_filename)\n",
    "\n",
    "if not os.path.exists(npz_path):\n",
    "    print(f\"Error: Embeddings file {npz_path} not found\")\n",
    "elif not os.path.exists(metadata_path):\n",
    "    print(f\"Error: Metadata file {metadata_path} not found\")\n",
    "else:\n",
    "    print(f\"Found embeddings file: {npz_path}\")\n",
    "    print(f\"Found metadata file: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if the files exist\n",
    "if os.path.exists(npz_path) and os.path.exists(metadata_path):\n",
    "    # Load embeddings from NPZ file\n",
    "    print(f\"Loading embeddings from {npz_path}...\")\n",
    "    embeddings = np.load(npz_path)\n",
    "    \n",
    "    # Load metadata\n",
    "    print(f\"Loading metadata from {metadata_path}...\")\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    # Get the number of embeddings\n",
    "    num_embeddings = len(metadata)\n",
    "    print(f\"Loaded {num_embeddings} embeddings for collection '{collection_name}'\")\n",
    "    \n",
    "    # Display a sample embedding\n",
    "    if embeddings.files:\n",
    "        sample_key = embeddings.files[0]\n",
    "        sample_embedding = embeddings[sample_key]\n",
    "        print(f\"\\nSample embedding for {sample_key}:\")\n",
    "        print(f\"Shape: {sample_embedding.shape}\")\n",
    "        print(f\"First 5 values: {sample_embedding[:5]}\")\n",
    "        \n",
    "        # Display corresponding metadata\n",
    "        if sample_key in metadata:\n",
    "            print(f\"\\nMetadata for {sample_key}:\")\n",
    "            for k, v in metadata[sample_key].items():\n",
    "                print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if embeddings and metadata were loaded successfully\n",
    "if 'embeddings' in locals() and 'metadata' in locals():\n",
    "    # Check if Qdrant collection exists, create if not\n",
    "    if not check_collection_exists(COLLECTION_NAME):\n",
    "        print(f\"Collection {COLLECTION_NAME} does not exist in Qdrant, creating it...\")\n",
    "        # Get vector size from the first embedding\n",
    "        first_key = list(embeddings.keys())[0]\n",
    "        vector_size = embeddings[first_key].shape[0]\n",
    "        create_collection(COLLECTION_NAME, vector_size)\n",
    "    else:\n",
    "        print(f\"Collection {COLLECTION_NAME} already exists in Qdrant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if embeddings and metadata were loaded successfully\n",
    "if 'embeddings' in locals() and 'metadata' in locals():\n",
    "    # Prepare batches for upload\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    \n",
    "    # Process all embeddings\n",
    "    for object_id, embedding_data in metadata.items():\n",
    "        # Skip if object_id is not in the embeddings file\n",
    "        if object_id not in embeddings:\n",
    "            print(f\"Warning: Object ID {object_id} not found in embeddings file, skipping\")\n",
    "            continue\n",
    "            \n",
    "        # Get the embedding vector\n",
    "        vector = embeddings[object_id]\n",
    "        \n",
    "        # Flatten the vector if it's 2D (e.g., shape (1, 768))\n",
    "        if len(vector.shape) > 1:\n",
    "            vector = vector.flatten()\n",
    "        \n",
    "        # Convert to list for JSON serialization\n",
    "        vector = vector.tolist()\n",
    "        \n",
    "        # Print debug info for the first few points\n",
    "        if len(current_batch) < 3:\n",
    "            print(f\"\\nDebug - Point {len(current_batch)+1}:\")\n",
    "            print(f\"ID: {object_id}\")\n",
    "            print(f\"Vector type: {type(vector)}\")\n",
    "            print(f\"Vector shape before flattening: {embeddings[object_id].shape}\")\n",
    "            print(f\"Vector length after flattening: {len(vector)}\")\n",
    "            print(f\"Payload: {embedding_data}\")\n",
    "        \n",
    "        # Create point for Qdrant\n",
    "        point = {\n",
    "            \"id\": hex_to_uuid(object_id),\n",
    "            \"vector\": vector,\n",
    "            \"payload\": {\n",
    "                \"object_id\": object_id,\n",
    "                \"collection_id\": embedding_data.get(\"nft_collection_name\", \"\"),\n",
    "                \"name\": embedding_data.get(\"name\", \"\"),\n",
    "                \"image_url\": embedding_data.get(\"image_url\", \"\")\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        current_batch.append(point)\n",
    "        \n",
    "        # If batch is full, add to batches list\n",
    "        if len(current_batch) >= BATCH_SIZE:\n",
    "            batches.append(current_batch)\n",
    "            current_batch = []\n",
    "    \n",
    "    # Add any remaining points to the batches list\n",
    "    if current_batch:\n",
    "        batches.append(current_batch)\n",
    "    \n",
    "    print(f\"Prepared {len(batches)} batches for upload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if batches were prepared successfully\n",
    "if 'batches' in locals() and batches:\n",
    "    # Upload batches in parallel\n",
    "    task_queue = queue.Queue()\n",
    "    for batch in batches:\n",
    "        task_queue.put(batch)\n",
    "    \n",
    "    # Add sentinel values to stop workers\n",
    "    for _ in range(MAX_WORKERS):\n",
    "        task_queue.put(None)\n",
    "    \n",
    "    # Initialize progress tracking\n",
    "    progress = {\n",
    "        \"success\": 0,\n",
    "        \"failed\": 0,\n",
    "        \"lock\": threading.Lock(),\n",
    "        \"pbar\": tqdm(total=num_embeddings, desc=f\"Uploading '{collection_name}' to Qdrant\")\n",
    "    }\n",
    "    \n",
    "    # Start workers\n",
    "    workers = []\n",
    "    for _ in range(MAX_WORKERS):\n",
    "        worker = threading.Thread(target=upload_worker, args=(COLLECTION_NAME, task_queue, progress))\n",
    "        worker.start()\n",
    "        workers.append(worker)\n",
    "    \n",
    "    # Wait for all tasks to complete\n",
    "    task_queue.join()\n",
    "    \n",
    "    # Wait for all workers to finish\n",
    "    for worker in workers:\n",
    "        worker.join()\n",
    "    \n",
    "    # Close progress bar\n",
    "    progress[\"pbar\"].close()\n",
    "    \n",
    "    print(f\"\\nUpload complete for collection '{collection_name}'!\")\n",
    "    print(f\"Successfully uploaded {progress['success']} points\")\n",
    "    print(f\"Failed to upload {progress['failed']} points\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if check_collection_exists(COLLECTION_NAME):\n",
    "    # Get collection info\n",
    "    url = f\"{QDRANT_API_URL}/collections/{COLLECTION_NAME}\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": QDRANT_API_KEY\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            collection_info = response.json()\n",
    "            points_count = collection_info.get('result', {}).get('vectors_count', 0)\n",
    "            print(f\"Collection {COLLECTION_NAME} contains {points_count} points\")\n",
    "        else:\n",
    "            print(f\"Error getting collection info: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error verifying upload: {str(e)}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
