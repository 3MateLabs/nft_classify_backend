{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### CELL 1: Import Libraries and Setup ##########\n",
    "\"\"\"\n",
    "Script to generate embeddings for a specific NFT collection and store them efficiently in a single NPZ file\n",
    "This approach processes one collection at a time for better manageability\n",
    "\"\"\"\n",
    "\n",
    "# Import required libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import sys\n",
    "import concurrent.futures\n",
    "import argparse\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Add the project root to the path so we can import from api\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "try:\n",
    "    from api.config import EMBEDDING_DIR\n",
    "except ImportError:\n",
    "    # Fallback if import fails\n",
    "    EMBEDDING_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), \"data\", \"embeddings\")\n",
    "    print(f\"Using fallback EMBEDDING_DIR: {EMBEDDING_DIR}\")\n",
    "\n",
    "# Create embedding directory if it doesn't exist\n",
    "os.makedirs(EMBEDDING_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image embedding service URL and API key\n",
    "IMAGE_EMBEDDING_API_URL = \"http://localhost:3001\"\n",
    "IMAGE_EMBEDDING_API_KEY = \"45334ad61f254307a32\"  # From memory\n",
    "\n",
    "# Constants\n",
    "MAX_WORKERS = 10  # Number of concurrent workers for parallel processing\n",
    "BATCH_SIZE = 50   # Batch size for saving embeddings to reduce memory usage\n",
    "\n",
    "# Default CSV path for NFT data\n",
    "DEFAULT_CSV_PATH = '2_all_nft_datas.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embedding_from_url(image_url: str) -> Optional[List[float]]:\n",
    "    \"\"\"\n",
    "    Generate embedding for an image using the image embedding service\n",
    "    \n",
    "    Args:\n",
    "        image_url: URL of the image to generate embedding for\n",
    "        \n",
    "    Returns:\n",
    "        List of embedding values or None if failed\n",
    "    \"\"\"\n",
    "    url = f\"{IMAGE_EMBEDDING_API_URL}/embed_from_url\"\n",
    "    \n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"X-API-KEY\": IMAGE_EMBEDDING_API_KEY\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"img_url\": image_url\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        \n",
    "        # Print response status for debugging if there's an error\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error response for {image_url}: Status {response.status_code} - {response.text}\")\n",
    "            return None\n",
    "            \n",
    "        data = response.json()\n",
    "        if \"embedding\" in data:\n",
    "            return data[\"embedding\"]\n",
    "        else:\n",
    "            print(f\"No embedding found in response for {image_url}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating embedding for {image_url}: {str(e)}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_nft(nft: Dict[str, Any]) -> Tuple[bool, str, Optional[List[float]]]:\n",
    "    \"\"\"\n",
    "    Process a single NFT to generate its embedding\n",
    "    \n",
    "    Args:\n",
    "        nft: Dictionary containing NFT data\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (success, object_id, embedding)\n",
    "    \"\"\"\n",
    "    # Handle different possible column names\n",
    "    object_id = nft.get(\"object_id\")\n",
    "    collection_name = nft.get(\"nft_collection_name\")\n",
    "    name = nft.get(\"name\", \"\")\n",
    "    image_url = nft.get(\"image_url\")\n",
    "    \n",
    "    if not object_id:\n",
    "        print(\"Error: NFT missing object_id\")\n",
    "        return False, \"\", None\n",
    "    \n",
    "    if not image_url:\n",
    "        print(f\"No image URL found for NFT {object_id}\")\n",
    "        return False, object_id, None\n",
    "    \n",
    "    # Generate embedding\n",
    "    embedding = generate_embedding_from_url(image_url)\n",
    "    \n",
    "    if embedding is None:\n",
    "        print(f\"Failed to generate embedding for NFT {object_id}\")\n",
    "        return False, object_id, None\n",
    "    \n",
    "    return True, object_id, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########## CELL 5: Batch Saving Function #########\n",
    "def save_embeddings_batch(collection_name: str, embeddings_dict: Dict[str, List[float]], \n",
    "                         metadata_dict: Dict[str, Dict[str, Any]], mode: str = 'a') -> None:\n",
    "    \"\"\"\n",
    "    Save a batch of embeddings to the NPZ file and update metadata\n",
    "    \n",
    "    Args:\n",
    "        collection_name: Name of the NFT collection\n",
    "        embeddings_dict: Dictionary of object_id -> embedding\n",
    "        metadata_dict: Dictionary of object_id -> metadata\n",
    "        mode: 'w' for write (overwrite), 'a' for append (default)\n",
    "    \"\"\"\n",
    "    # Create filenames based on collection name\n",
    "    npz_filename = f\"{collection_name}_embeddings.npz\"\n",
    "    metadata_filename = f\"{collection_name}_metadata.json\"\n",
    "    \n",
    "    npz_path = os.path.join(EMBEDDING_DIR, npz_filename)\n",
    "    metadata_path = os.path.join(EMBEDDING_DIR, metadata_filename)\n",
    "    \n",
    "    # Convert embeddings to numpy arrays\n",
    "    np_embeddings = {}\n",
    "    for object_id, embedding in embeddings_dict.items():\n",
    "        np_embeddings[object_id] = np.array(embedding)\n",
    "    \n",
    "    # Save embeddings to NPZ file\n",
    "    if mode == 'w' or not os.path.exists(npz_path):\n",
    "        # Create new NPZ file\n",
    "        np.savez_compressed(npz_path, **np_embeddings)\n",
    "    else:\n",
    "        # Append to existing NPZ file\n",
    "        # Since NPZ doesn't support direct append, we need to load existing data,\n",
    "        # merge with new data, and save everything back\n",
    "        existing_data = dict(np.load(npz_path))\n",
    "        existing_data.update(np_embeddings)\n",
    "        np.savez_compressed(npz_path, **existing_data)\n",
    "    \n",
    "    # Update metadata file\n",
    "    if os.path.exists(metadata_path):\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            existing_metadata = json.load(f)\n",
    "    else:\n",
    "        existing_metadata = {}\n",
    "    \n",
    "    existing_metadata.update(metadata_dict)\n",
    "    \n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(existing_metadata, f)\n",
    "    \n",
    "    print(f\"Saved batch of {len(embeddings_dict)} embeddings to {npz_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collection_nfts(collection_name: str, csv_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Get NFTs for a specific collection from the CSV file\n",
    "    \n",
    "    Args:\n",
    "        collection_name: Name of the collection to filter by\n",
    "        csv_path: Path to the CSV file with NFT data\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing NFT data\n",
    "    \"\"\"\n",
    "    # Load NFT data from CSV file\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"Error: NFT data file {csv_path} not found\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        nfts_df = pd.read_csv(csv_path)\n",
    "        print(f\"Loaded CSV with columns: {list(nfts_df.columns)}\")\n",
    "        \n",
    "        # Check if 'nft_collection_name' column exists\n",
    "        collection_column = None\n",
    "        if 'nft_collection_name' in nfts_df.columns:\n",
    "            collection_column = 'nft_collection_name'\n",
    "        elif 'nftCollectionName' in nfts_df.columns:\n",
    "            collection_column = 'nftCollectionName'\n",
    "        \n",
    "        if collection_column is None:\n",
    "            print(\"Error: Could not find collection name column in CSV\")\n",
    "            return []\n",
    "        \n",
    "        # Filter by collection name\n",
    "        collection_nfts = nfts_df[nfts_df[collection_column] == collection_name]\n",
    "        \n",
    "        print(f\"Found {len(collection_nfts)} NFTs for collection '{collection_name}'\")\n",
    "        \n",
    "        # Convert DataFrame to list of dictionaries\n",
    "        return collection_nfts.to_dict('records')\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def list_collections(csv_path: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    List all collections in the CSV file\n",
    "    \n",
    "    Args:\n",
    "        csv_path: Path to the CSV file with NFT data\n",
    "        \n",
    "    Returns:\n",
    "        List of collection names\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(f\"Error: NFT data file {csv_path} not found\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        nfts_df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # Check if 'nft_collection_name' column exists\n",
    "        collection_column = None\n",
    "        if 'nft_collection_name' in nfts_df.columns:\n",
    "            collection_column = 'nft_collection_name'\n",
    "        elif 'nftCollectionName' in nfts_df.columns:\n",
    "            collection_column = 'nftCollectionName'\n",
    "        \n",
    "        if collection_column is None:\n",
    "            print(\"Error: Could not find collection name column in CSV\")\n",
    "            return []\n",
    "            \n",
    "        collections = nfts_df[collection_column].unique().tolist()\n",
    "        return collections\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {str(e)}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the CSV path - adjust this to your file location\n",
    "csv_path = DEFAULT_CSV_PATH\n",
    "if not os.path.isabs(csv_path):\n",
    "    csv_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), csv_path)\n",
    "\n",
    "# List all available collections\n",
    "collections = list_collections(csv_path)\n",
    "print(f\"\\nAvailable collections ({len(collections)}):\\n\")\n",
    "for i, collection in enumerate(collections):\n",
    "    print(f\"{i+1}. {collection}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the collection name here - replace with your desired collection\n",
    "collection_name = \"DoubleUp Citizen\"  # Example - change this to your target collection\n",
    "\n",
    "# Get NFTs for the specified collection\n",
    "nfts = get_collection_nfts(collection_name, csv_path)\n",
    "\n",
    "if not nfts:\n",
    "    print(f\"No NFTs found for collection '{collection_name}'\")\n",
    "else:\n",
    "    print(f\"Found {len(nfts)} NFTs for collection '{collection_name}'\")\n",
    "    \n",
    "    # Display a sample NFT to verify data\n",
    "    print(\"\\nSample NFT data:\")\n",
    "    sample_nft = nfts[0]\n",
    "    for key, value in sample_nft.items():\n",
    "        print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create filenames based on collection name\n",
    "npz_filename = f\"{collection_name}_embeddings.npz\"\n",
    "metadata_filename = f\"{collection_name}_metadata.json\"\n",
    "\n",
    "npz_path = os.path.join(EMBEDDING_DIR, npz_filename)\n",
    "metadata_path = os.path.join(EMBEDDING_DIR, metadata_filename)\n",
    "\n",
    "# Check if files already exist\n",
    "overwrite_existing = False\n",
    "if os.path.exists(npz_path) and os.path.exists(metadata_path):\n",
    "    print(f\"Files already exist for collection '{collection_name}'\")\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        existing_metadata = json.load(f)\n",
    "    print(f\"Existing metadata contains {len(existing_metadata)} entries\")\n",
    "    \n",
    "    # In notebook, we'll set this manually rather than prompting\n",
    "    overwrite_existing = True  # Set to True to overwrite, False to keep existing\n",
    "    \n",
    "    if overwrite_existing:\n",
    "        # Remove existing files\n",
    "        os.remove(npz_path)\n",
    "        os.remove(metadata_path)\n",
    "        print(\"Removed existing files\")\n",
    "    else:\n",
    "        print(\"Keeping existing files - processing will be skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run this cell if we have NFTs and either no existing files or we want to overwrite\n",
    "if nfts and (not os.path.exists(npz_path) or overwrite_existing):\n",
    "    # Initialize counters and storage\n",
    "    success_count = 0\n",
    "    failed_count = 0\n",
    "    failed_nfts = []\n",
    "    \n",
    "    # Initialize batch storage\n",
    "    batch_embeddings = {}\n",
    "    batch_metadata = {}\n",
    "    batch_count = 0\n",
    "    \n",
    "    # Process NFTs in parallel\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_nft = {executor.submit(process_nft, nft): nft for nft in nfts}\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in tqdm(concurrent.futures.as_completed(future_to_nft), total=len(nfts), \n",
    "                          desc=f\"Processing '{collection_name}' NFTs\"):\n",
    "            nft = future_to_nft[future]\n",
    "            try:\n",
    "                success, object_id, embedding = future.result()\n",
    "                \n",
    "                if success and embedding is not None:\n",
    "                    # Add to batch\n",
    "                    batch_embeddings[object_id] = embedding\n",
    "                    \n",
    "                    # Store metadata\n",
    "                    batch_metadata[object_id] = {\n",
    "                        \"object_id\": object_id,\n",
    "                        \"nft_collection_name\": nft.get(\"nft_collection_name\"),\n",
    "                        \"name\": nft.get(\"name\", \"\"),\n",
    "                        \"image_url\": nft.get(\"image_url\"),\n",
    "                        \"embedding_dimensions\": len(embedding)\n",
    "                    }\n",
    "                    \n",
    "                    batch_count += 1\n",
    "                    success_count += 1\n",
    "                    \n",
    "                    # Save batch if it reaches the batch size\n",
    "                    if batch_count >= BATCH_SIZE:\n",
    "                        save_embeddings_batch(collection_name, batch_embeddings, batch_metadata)\n",
    "                        batch_embeddings = {}\n",
    "                        batch_metadata = {}\n",
    "                        batch_count = 0\n",
    "                else:\n",
    "                    failed_count += 1\n",
    "                    failed_nfts.append({\n",
    "                        \"object_id\": object_id,\n",
    "                        \"nft_collection_name\": nft.get(\"nft_collection_name\"),\n",
    "                        \"name\": nft.get(\"name\", \"\"),\n",
    "                        \"image_url\": nft.get(\"image_url\")\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing NFT {nft.get('object_id')}: {str(e)}\")\n",
    "                failed_count += 1\n",
    "                failed_nfts.append({\n",
    "                    \"object_id\": nft.get(\"object_id\", \"\"),\n",
    "                    \"nft_collection_name\": nft.get(\"nft_collection_name\", \"\"),\n",
    "                    \"name\": nft.get(\"name\", \"\"),\n",
    "                    \"image_url\": nft.get(\"image_url\", \"\"),\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "    \n",
    "    # Save any remaining embeddings in the batch\n",
    "    if batch_count > 0:\n",
    "        save_embeddings_batch(collection_name, batch_embeddings, batch_metadata)\n",
    "    \n",
    "    # Save failed NFTs to CSV\n",
    "    if failed_nfts:\n",
    "        failed_df = pd.DataFrame(failed_nfts)\n",
    "        failed_csv_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \n",
    "                                      f\"{collection_name}_failed_nfts.csv\")\n",
    "        failed_df.to_csv(failed_csv_path, index=False)\n",
    "        print(f\"Failed NFTs saved to {failed_csv_path}\")\n",
    "    \n",
    "    print(f\"\\nProcessing complete for collection '{collection_name}'!\")\n",
    "    print(f\"Successfully generated embeddings for {success_count} NFTs\")\n",
    "    print(f\"Failed to generate embeddings for {failed_count} NFTs\")\n",
    "    print(f\"All embeddings saved to {npz_path}\")\n",
    "    print(f\"Metadata saved to {metadata_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the generated embeddings and metadata\n",
    "if os.path.exists(npz_path) and os.path.exists(metadata_path):\n",
    "    # Load the NPZ file\n",
    "    embeddings = np.load(npz_path)\n",
    "    \n",
    "    # Load the metadata\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(f\"Embeddings file contains {len(embeddings.files)} embeddings\")\n",
    "    print(f\"Metadata file contains {len(metadata)} entries\")\n",
    "    \n",
    "    # Display a sample embedding\n",
    "    if embeddings.files:\n",
    "        sample_key = embeddings.files[0]\n",
    "        sample_embedding = embeddings[sample_key]\n",
    "        print(f\"\\nSample embedding for {sample_key}:\")\n",
    "        print(f\"Shape: {sample_embedding.shape}\")\n",
    "        print(f\"First 5 values: {sample_embedding[:5]}\")\n",
    "        \n",
    "        # Display corresponding metadata\n",
    "        if sample_key in metadata:\n",
    "            print(f\"\\nMetadata for {sample_key}:\")\n",
    "            for k, v in metadata[sample_key].items():\n",
    "                print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No embeddings or metadata files found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
