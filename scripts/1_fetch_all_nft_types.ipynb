{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "import os\n",
    "\n",
    "# TradePort API credentials\n",
    "API_USER = \"cyber\"\n",
    "API_KEY = \"Ep6xO5U.974416bcaa0b8b0e08f15f5f03005cf7\"\n",
    "API_ENDPOINT = \"https://api.indexer.xyz/graphql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_verified_nft_collections(offset: int = 0) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetch verified NFT collections from TradePort API with a specific offset\n",
    "    \n",
    "    Args:\n",
    "        offset: The offset to start fetching from\n",
    "        \n",
    "    Returns:\n",
    "        List of collection dictionaries\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"x-api-user\": API_USER,\n",
    "        \"x-api-key\": API_KEY,\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Query for verified collections with fixed limit of 25\n",
    "    query = \"\"\"\n",
    "    query {\n",
    "      sui {\n",
    "        collections(where: {verified: {_eq: true}}, limit: 25, offset: %d) {\n",
    "          title\n",
    "          slug\n",
    "          verified\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\" % offset\n",
    "    \n",
    "    try:\n",
    "        print(f\"Fetching verified NFT collections with offset {offset}...\")\n",
    "        response = requests.post(\n",
    "            API_ENDPOINT,\n",
    "            headers=headers,\n",
    "            json={\"query\": query}\n",
    "        )\n",
    "        \n",
    "        # Print response for debugging\n",
    "        print(f\"Response status code: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            print(response.text)\n",
    "            return []\n",
    "        \n",
    "        data = response.json()\n",
    "        \n",
    "        # Check if we have collections in the response\n",
    "        collections = data.get(\"data\", {}).get(\"sui\", {}).get(\"collections\", [])\n",
    "        \n",
    "        print(f\"Fetched {len(collections)} collections with offset {offset}\")\n",
    "        return collections\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching collections: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing CSV file: 1_all_nft_types.csv\n",
      "Existing file contains 145 collections\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"1_all_nft_types.csv\"\n",
    "existing_df = None\n",
    "\n",
    "if os.path.exists(csv_path):\n",
    "    print(f\"Found existing CSV file: {csv_path}\")\n",
    "    existing_df = pd.read_csv(csv_path)\n",
    "    print(f\"Existing file contains {len(existing_df)} collections\")\n",
    "else:\n",
    "    print(f\"No existing CSV file found at {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching verified NFT collections with offset 0...\n",
      "Error fetching collections: HTTPSConnectionPool(host='api.indexer.xyz', port=443): Max retries exceeded with url: /graphql (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x10fd40d00>: Failed to resolve 'api.indexer.xyz' ([Errno 8] nodename nor servname provided, or not known)\"))\n",
      "Fetching verified NFT collections with offset 25...\n",
      "Error fetching collections: HTTPSConnectionPool(host='api.indexer.xyz', port=443): Max retries exceeded with url: /graphql (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x10fd40250>: Failed to resolve 'api.indexer.xyz' ([Errno 8] nodename nor servname provided, or not known)\"))\n",
      "Fetching verified NFT collections with offset 50...\n",
      "Error fetching collections: HTTPSConnectionPool(host='api.indexer.xyz', port=443): Max retries exceeded with url: /graphql (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x106e7cd00>: Failed to resolve 'api.indexer.xyz' ([Errno 8] nodename nor servname provided, or not known)\"))\n",
      "Total collections fetched in first batch: 0\n"
     ]
    }
   ],
   "source": [
    "all_collections = []\n",
    "\n",
    "# First batch\n",
    "for offset in [0, 25, 50]:\n",
    "    collections = fetch_verified_nft_collections(offset)\n",
    "    all_collections.extend(collections)\n",
    "    # Add a small delay to avoid rate limiting\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"Total collections fetched in first batch: {len(all_collections)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 145 existing slugs to the seen set\n",
      "New unique verified collections: 0\n"
     ]
    }
   ],
   "source": [
    "for offset in [75, 100, 125, 150, 175, 200]:\n",
    "    collections = fetch_verified_nft_collections(offset)\n",
    "    all_collections.extend(collections)\n",
    "    # Add a small delay to avoid rate limiting\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"Total collections fetched after second batch: {len(all_collections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No new verified collections found.\n",
      "Keeping existing 145 collections in the CSV file.\n"
     ]
    }
   ],
   "source": [
    "unique_collections = []\n",
    "seen_slugs = set()\n",
    "\n",
    "# If we have existing data, add those slugs to the seen set\n",
    "if existing_df is not None:\n",
    "    for slug in existing_df[\"nft_type\"]:\n",
    "        seen_slugs.add(slug)\n",
    "    print(f\"Added {len(seen_slugs)} existing slugs to the seen set\")\n",
    "\n",
    "# Process new collections\n",
    "for collection in all_collections:\n",
    "    slug = collection.get(\"slug\", \"\")\n",
    "    if slug and slug not in seen_slugs:\n",
    "        seen_slugs.add(slug)\n",
    "        unique_collections.append(collection)\n",
    "\n",
    "print(f\"New unique verified collections: {len(unique_collections)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No collections to save.\n",
      "Data saved to 1_all_nft_types.csv\n",
      "Total collections in CSV: 145\n"
     ]
    }
   ],
   "source": [
    "if not unique_collections:\n",
    "    print(\"No new verified collections found.\")\n",
    "    if existing_df is not None:\n",
    "        print(f\"Keeping existing {len(existing_df)} collections in the CSV file.\")\n",
    "    new_df = pd.DataFrame(columns=[\"nft_collection_name\", \"nft_type\", \"verified\", \"tradeport_url\"])\n",
    "else:\n",
    "    # Process collections into a DataFrame\n",
    "    processed_data = []\n",
    "    \n",
    "    for collection in unique_collections:\n",
    "        # For nft_type, we'll use the slug as a fallback since we couldn't get the type directly\n",
    "        slug = collection.get(\"slug\", \"\")\n",
    "        processed_data.append({\n",
    "            \"nft_collection_name\": collection.get(\"title\", \"\"),\n",
    "            \"nft_type\": slug,  # Using slug as a fallback for type\n",
    "            \"verified\": collection.get(\"verified\", True),  # Should always be True\n",
    "            \"tradeport_url\": f\"https://www.tradeport.xyz/sui/collection/{slug}\"\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame for new collections\n",
    "    new_df = pd.DataFrame(processed_data)\n",
    "    print(f\"Created DataFrame with {len(new_df)} new collections\")\n",
    "\n",
    "# Display first few rows of new data\n",
    "if not new_df.empty:\n",
    "    print(\"\\nFirst 5 rows of the new data:\")\n",
    "    display(new_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if existing_df is not None and not new_df.empty:\n",
    "    merged_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "    print(f\"\\nTotal verified collections after merge: {len(merged_df)}\")\n",
    "    nft_df = merged_df\n",
    "else:\n",
    "    if not new_df.empty:\n",
    "        print(f\"\\nTotal new verified collections: {len(new_df)}\")\n",
    "        nft_df = new_df\n",
    "    else:\n",
    "        print(\"No collections to save.\")\n",
    "        nft_df = existing_df if existing_df is not None else pd.DataFrame(\n",
    "            columns=[\"nft_collection_name\", \"nft_type\", \"verified\", \"tradeport_url\"]\n",
    "        )\n",
    "\n",
    "# Save to CSV if we have data\n",
    "if not nft_df.empty:\n",
    "    nft_df.to_csv(csv_path, index=False)\n",
    "    print(f\"Data saved to {csv_path}\")\n",
    "    print(f\"Total collections in CSV: {len(nft_df)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
